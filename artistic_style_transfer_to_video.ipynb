{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ZFH_3.0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUdcaKQa8FJe",
        "colab_type": "text"
      },
      "source": [
        "# Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aSgAs6p2TjcP",
        "colab": {}
      },
      "source": [
        "# we need pillow version of 5.3.0\n",
        "# we will uninstall the older version first\n",
        "!pip uninstall -y Pillow\n",
        "# install the new one\n",
        "!pip install Pillow==5.3.0\n",
        "# import the new one\n",
        "import PIL\n",
        "print(PIL.PILLOW_VERSION)\n",
        "# this should print 5.3.0. If it doesn't, then restart your runtime:\n",
        "# Menu > Runtime > Restart Runtime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iygZP03eOymf",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZAuc2thwQuvO",
        "colab": {}
      },
      "source": [
        "cd '/content/drive/My Drive/Neural Style Transfer for Videos/ZFH'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L_sDghy4RpPj",
        "colab": {}
      },
      "source": [
        "# google colab does not come with torch installed\n",
        "\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "# !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NTdz1AOVR-rE",
        "colab": {}
      },
      "source": [
        "# we will verify that GPU is enabled for this notebook\n",
        "# following should print: CUDA is available!  Training on GPU ...\n",
        "# \n",
        "# if it prints otherwise, then you need to enable GPU: \n",
        "# from Menu > Runtime > Change Runtime Type > Hardware Accelerator > GPU\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYSyIj8aIO7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import PIL\n",
        "print(PIL.PILLOW_VERSION)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mtMBAFjRN2II",
        "colab": {}
      },
      "source": [
        "# Import the necessary packages:\n",
        "import cv2\n",
        "import glob\n",
        "from PIL import Image  # Python Image Library for image processing\n",
        "import matplotlib.pyplot as plt  # Plotting\n",
        "import numpy as np  # Numerical computation\n",
        "import torch  # Neural network computation\n",
        "from torch import optim  # optimizer to minimize the loss function\n",
        "from torchvision import transforms, models  # Transformations on images and pre-trained models\n",
        "import os, os.path  # To count the number of image files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZpVE0dcjN5TJ",
        "colab": {}
      },
      "source": [
        "# LOAD VGG19 (features only):\n",
        "# vgg19.features \t: It consists of all the convolutional and pooling layers\n",
        "# vgg19.classifier \t: It consists of the 3 linear classifier layers at the end\n",
        "\n",
        "# We load in the pre-trained model and freeze the weights:\n",
        "styleTransferModel = models.vgg19(pretrained=True).features\n",
        "\n",
        "# Freeze all the VGG parameters since we are only optimizing the target image:\n",
        "for params in styleTransferModel.parameters():\n",
        "\tparams.requires_grad_(False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "br-SbR_BN_S9",
        "colab": {}
      },
      "source": [
        "# Check if GPU is available:\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Running on GPU\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Running on CPU\")\n",
        "\n",
        "# Move the model to the GPU if available:\n",
        "styleTransferModel.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dyo-yDUJOBTF",
        "colab": {}
      },
      "source": [
        "# Load the content and style images:\n",
        "def load_image(image_path, max_size=480, shape=None):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "    # Resize the image as a large image will slow down processing:\n",
        "    if max(image.size) > max_size:\n",
        "        img_size = max_size\n",
        "    else:\n",
        "        img_size = max(image.size)\n",
        "\n",
        "    if shape is not None:\n",
        "        img_size = shape\n",
        "\n",
        "    img_transform = transforms.Compose([\n",
        "        transforms.Resize(img_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406),\n",
        "                             (0.229, 0.224, 0.225))])\n",
        "    # Discard the transparent alpha channel (that's the :3) and add the batch dimension:\n",
        "    image = img_transform(image)[:3, :, :].unsqueeze(0)\n",
        "\n",
        "    return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3ZQJkUfLOEzW",
        "colab": {}
      },
      "source": [
        "# CONTENT AND STYLE FEATURES:\n",
        "# Map the layer names to the names given in the paper:\n",
        "def get_features(image, model, layers=None):\n",
        "    if layers is None:\n",
        "        layers = {'0': 'conv1_1',\n",
        "                  '5': 'conv2_1',\n",
        "                  '10': 'conv3_1',\n",
        "                  '19': 'conv4_1',\n",
        "                  '21': 'conv4_2',  # Content representation\n",
        "                  '28': 'conv5_1',}\n",
        "\n",
        "    features = {}\n",
        "    x = image\n",
        "    for name, layer in model._modules.items():\n",
        "        x = layer(x)\n",
        "        if name in layers:\n",
        "            features[layers[name]] = x\n",
        "\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "umMxhudCOHbc",
        "colab": {}
      },
      "source": [
        "# GRAM MATRIX:\n",
        "def gram_matrix(tensor):\n",
        "  batch_size, depth, height, width = tensor.size()\n",
        "  # Vectorize the input image tensor and add all the feature maps:\n",
        "  tensor = tensor.view(depth, height * width)\n",
        "  # Transpose the image tensor:\n",
        "  tensor_t = tensor.t()\n",
        "  # Compute the gram matrix by multiplying the matrix by its transpose:\n",
        "  gram = torch.mm(tensor, tensor_t)\n",
        "\n",
        "  return gram"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "apdv4c_gOJh8",
        "colab": {}
      },
      "source": [
        "def convert_video_to_frames(video_directory):\n",
        "    # Load the input video:\n",
        "    capture_video = cv2.VideoCapture(video_directory)\n",
        "    # Read the input frame:\n",
        "    success, frame = capture_video.read()\n",
        "    # Set counter for number of frames read:\n",
        "    count = 1\n",
        "\n",
        "    while success:\n",
        "        # Save the frame that is read:\n",
        "        cv2.imwrite('input_content_frames/frame_%d.jpg' % count, frame)\n",
        "        # Read the next input frame:\n",
        "        success, frame = capture_video.read()\n",
        "        print(\"Read a new frame: \", count, \"Success: \", success)\n",
        "        count += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tXY8GSCdONr0",
        "colab": {}
      },
      "source": [
        "# Split the video into frames:\n",
        "convert_video_to_frames('input_video/IMG_6039.MOV')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3th2rfaYOPqt",
        "colab": {}
      },
      "source": [
        "# Function to un-normalize an image and convert from a Tensor image to a NumPy image for display or writing to disk:\n",
        "def img_convert(tensor):\n",
        "    # Display a tensor as an image:\n",
        "\n",
        "    image = tensor.to(\"cpu\").clone().detach()\n",
        "    image = image.numpy().squeeze()\n",
        "    image = image.transpose(1, 2, 0)\n",
        "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
        "    image = image.clip(0, 1)\n",
        "\n",
        "    return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUl4Z43-zMx0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/pathak22/pyflow.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AP-TQJuEYPI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd pyflow/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRBoNdWwYPg6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python setup.py build_ext -i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qqexvm6qzOEt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "# from __future__ import unicode_literals\n",
        "import time\n",
        "import argparse\n",
        "import pyflow\n",
        "\n",
        "def optical_flow(Image1, Image2, Image1_index, Image2_index):\n",
        "  im1 = np.array(Image1, order='C').astype(float)\n",
        "  im2 = np.array(Image2, order='C').astype(float)\n",
        "\n",
        "  # Flow Options:\n",
        "  alpha = 0.012\n",
        "  ratio = 0.75\n",
        "  minWidth = 20\n",
        "  nOuterFPIterations = 7\n",
        "  nInnerFPIterations = 1\n",
        "  nSORIterations = 30\n",
        "  colType = 0  # 0 or default:RGB, 1:GRAY (but pass gray image with shape (h,w,1))\n",
        "\n",
        "  s = time.time()\n",
        "  u, v, im2W = pyflow.coarse2fine_flow(\n",
        "      im1, im2, alpha, ratio, minWidth, nOuterFPIterations, nInnerFPIterations,\n",
        "      nSORIterations, colType)\n",
        "  e = time.time()\n",
        "\n",
        "  flow = np.concatenate((u[..., None], v[..., None]), axis=2)\n",
        "  np.save('../flow/flow_matrix/outFlow_%s_%s.npy' %(Image1_index, Image2_index), flow)\n",
        "\n",
        "  hsv = np.zeros(im1.shape, dtype=np.uint8)\n",
        "  hsv[:, :, 0] = 255\n",
        "  hsv[:, :, 1] = 255\n",
        "  mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "  hsv[..., 0] = ang * 180 / np.pi / 2\n",
        "  hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
        "  rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
        "  cv2.imwrite('../flow/outFlow_%s_%s.png' %(Image1_index, Image2_index), rgb)\n",
        "  im_warped = im2W[:, :, ::-1] * 255\n",
        "  cv2.imwrite('../flow/Warped_%s_%s.jpg' %(Image1_index, Image2_index), im_warped)\n",
        "\n",
        "  return flow, im_warped"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXimqhyVdKU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_frames_length = len([name for name in os.listdir('../input_content_frames') if os.path.isfile(os.path.join('../input_content_frames', name))])\n",
        "frame_interval = (1,2,4)\n",
        "\n",
        "for frame in frame_interval:\n",
        "  for i in range(1, input_frames_length+1-frame):\n",
        "    del_1, del_2 = optical_flow(img_convert(load_image('../input_content_frames/frame_%s.jpg' %str(i))), img_convert(load_image('../input_content_frames/frame_%s.jpg' %str(i+frame))), str(i), str(i+frame))\n",
        "    print('Forward flow from %d to %d completed!' %(i, i+frame))\n",
        "  for i in range(input_frames_length, frame, -1):\n",
        "    del_1, del_2 = optical_flow(img_convert(load_image('../input_content_frames/frame_%s.jpg' %str(i))), img_convert(load_image('../input_content_frames/frame_%s.jpg' %str(i-frame))), str(i), str(i-frame))\n",
        "    print('Backward flow from %d to %d completed!' %(i, i-frame))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8s3lFrvNED8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def warped_flow_and_pixel_weight(Image1_index, Image2_index):\n",
        "  backward_flow = np.load('../flow/flow_matrix/outFlow_%s_%s.npy' %(Image2_index, Image1_index))\n",
        "  h, w, _ = backward_flow.shape\n",
        "  coordinate = np.rint(backward_flow + \n",
        "                       np.concatenate((np.repeat(np.arange(h)+1, w).reshape(h,w,1), \n",
        "                                       np.tile(np.arange(w)+1, h).reshape(h,w,1)), axis=2))\n",
        "  flow = []\n",
        "  forward_flow = np.load('../flow/flow_matrix/outFlow_%s_%s.npy' %(Image1_index, Image2_index))\n",
        "  for i in range(coordinate.shape[0]):\n",
        "    for j in range(coordinate.shape[1]):\n",
        "      x, y = coordinate[i,j]\n",
        "      flow.append(forward_flow[min(h-1,max(0,int(x))),min(w-1,max(0,int(y)))])\n",
        "  warped_flow = np.array(flow).reshape(h,w,-1)\n",
        "  np.save('../flow/flow_matrix/outFlow_%s_%s_warped.npy' %(Image1_index, Image2_index), warped_flow)\n",
        "  disocclusion = np.square(np.linalg.norm(warped_flow + backward_flow, axis=2)) - 0.01*(np.square(np.linalg.norm(warped_flow, axis=2))+np.square(np.linalg.norm(backward_flow, axis=2))) - 0.5 > 0\n",
        "  motion_boundary = np.square(np.gradient(backward_flow, axis=2)[:,:,0])+np.square(np.gradient(backward_flow, axis=2)[:,:,1]) - 0.01*np.square(np.linalg.norm(backward_flow, axis=2)) - 0.002 > 0\n",
        "  np.save('../flow/flow_matrix/pixel_weight_%s_%s.npy' %(Image1_index, Image2_index), (~(disocclusion + motion_boundary)).astype(int))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2usP62ApRK1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frame_interval = (1,2,4)\n",
        "for frame in frame_interval:\n",
        "  for i in range(1, input_frames_length+1-frame):\n",
        "    warped_flow_and_pixel_weight(i, i+frame)\n",
        "    print('Warped flow and pixel weigt between %d and %d completed!' %(i, i+frame))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anACF5VGLtgk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def long_pixel_weight(Image2_index):\n",
        "  if Image2_index == 2:\n",
        "    c_iminus1_i = np.load('../flow/flow_matrix/pixel_weight_%s_%s.npy' %(Image2_index-1, Image2_index))\n",
        "    np.save('../flow/flow_matrix/long_pixel_weight_%s_%s.npy' %(Image2_index-1, Image2_index), c_iminus1_i)\n",
        "  elif Image2_index == 3:\n",
        "    c_iminus1_i = np.load('../flow/flow_matrix/pixel_weight_%s_%s.npy' %(Image2_index-1, Image2_index))\n",
        "    np.save('../flow/flow_matrix/long_pixel_weight_%s_%s.npy' %(Image2_index-1, Image2_index), c_iminus1_i)\n",
        "    c_iminus2_i = np.load('../flow/flow_matrix/pixel_weight_%s_%s.npy' %(Image2_index-2, Image2_index))\n",
        "    np.save('../flow/flow_matrix/long_pixel_weight_%s_%s.npy' %(Image2_index-2, Image2_index), np.maximum(c_iminus2_i - c_iminus1_i, np.zeros(c_iminus1_i.shape)))\n",
        "  elif Image2_index == 4:\n",
        "    c_iminus1_i = np.load('../flow/flow_matrix/pixel_weight_%s_%s.npy' %(Image2_index-1, Image2_index))\n",
        "    np.save('../flow/flow_matrix/long_pixel_weight_%s_%s.npy' %(Image2_index-1, Image2_index), c_iminus1_i)\n",
        "    c_iminus2_i = np.load('../flow/flow_matrix/pixel_weight_%s_%s.npy' %(Image2_index-2, Image2_index))\n",
        "    np.save('../flow/flow_matrix/long_pixel_weight_%s_%s.npy' %(Image2_index-2, Image2_index), np.maximum(c_iminus2_i - c_iminus1_i, np.zeros(c_iminus1_i.shape)))\n",
        "  else:\n",
        "    c_iminus1_i = np.load('../flow/flow_matrix/pixel_weight_%s_%s.npy' %(Image2_index-1, Image2_index))\n",
        "    np.save('../flow/flow_matrix/long_pixel_weight_%s_%s.npy' %(Image2_index-1, Image2_index), c_iminus1_i)\n",
        "    c_iminus2_i = np.load('../flow/flow_matrix/pixel_weight_%s_%s.npy' %(Image2_index-2, Image2_index))\n",
        "    np.save('../flow/flow_matrix/long_pixel_weight_%s_%s.npy' %(Image2_index-2, Image2_index), np.maximum(c_iminus2_i - c_iminus1_i, np.zeros(c_iminus1_i.shape))) \n",
        "    c_iminus4_i = np.load('../flow/flow_matrix/pixel_weight_%s_%s.npy' %(Image2_index-4, Image2_index))\n",
        "    np.save('../flow/flow_matrix/long_pixel_weight_%s_%s.npy' %(Image2_index-4, Image2_index), np.maximum(c_iminus4_i - c_iminus2_i - c_iminus1_i, np.zeros(c_iminus1_i.shape)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nDDrpu6t_DR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for frame in range(2, input_frames_length+1):\n",
        "  long_pixel_weight(frame)\n",
        "  print('Long pixel weights for frame %d completed!' %frame)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMZp6q2r3FZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def warp_flow(Image1_index, Image2_index):\n",
        "  flow = np.load('../flow/flow_matrix/outFlow_%s_%s_warped.npy' %(Image1_index, Image2_index))\n",
        "  Image1 = img_convert(load_image('../output_style_transferred_frames/st_frame_%d.jpg' %Image1_index))\n",
        "  im1 = np.array(Image1, order='C').astype(float)\n",
        "  h, w = flow.shape[:2]\n",
        "  flow = -flow\n",
        "  flow[:,:,0] += np.arange(w)\n",
        "  flow[:,:,1] += np.arange(h)[:,np.newaxis]\n",
        "  flow = 2*flow/(flow.max(axis=(0,1)) - flow.min(axis=(0,1)))-1\n",
        "  warped_frame = torch.nn.functional.grid_sample(torch.from_numpy(im1)[np.newaxis, :].permute(0, 3, 1, 2), torch.from_numpy(flow)[np.newaxis, :])\n",
        "  plt.imsave('../input_content_frames/warped/frame_%s_%s_warped.jpg' %(Image1_index, Image2_index), warped_frame.permute(0, 2, 3, 1).squeeze().numpy())\n",
        "  # return warped_frame.float()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8ag1fOcQORyl",
        "colab": {}
      },
      "source": [
        "def apply_style_transfer(content_img_dir, style_img_dir, style_img_name, output_img_dir):\n",
        "    content_image_count = 1\n",
        "    \n",
        "    num_content_imgs = len([name for name in os.listdir('../%s' %content_img_dir) if os.path.isfile(os.path.join('../', content_img_dir, name))])\n",
        "    \n",
        "    while(content_image_count <= num_content_imgs):\n",
        "        content_image = load_image('../' + content_img_dir + '/frame_' + str(content_image_count) + '.jpg').to(device)\n",
        "\n",
        "        style_image = load_image('../' + style_img_dir + '/' + style_img_name, shape=content_image.shape[-2:]).to(device)\n",
        "\n",
        "        # Retrieve the features:\n",
        "        content_features = get_features(content_image, styleTransferModel)\n",
        "        style_features = get_features(style_image, styleTransferModel)\n",
        "\n",
        "        # Calculate the gram matrix for each of our style representations:\n",
        "        style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n",
        "\n",
        "        if content_image_count == 1:\n",
        "          target_image = content_image.clone().requires_grad_(True).to(device)\n",
        "          # Number of iterations to update your image:\n",
        "          steps = 3000\n",
        "          show_every = 200\n",
        "        else:\n",
        "          warp_flow(content_image_count-1, content_image_count)\n",
        "          if content_image_count-2 > 0:\n",
        "            warp_flow(content_image_count-2, content_image_count)\n",
        "          if content_image_count-4 > 0:\n",
        "            warp_flow(content_image_count-4, content_image_count)\n",
        "          target_image = load_image('../' + content_img_dir + '/warped/frame_%d_%d_warped.jpg' %(content_image_count-1, content_image_count)).to(device)\n",
        "          target_image = target_image.clone().requires_grad_(True).to(device)\n",
        "          # Number of iterations to update your image:\n",
        "          steps = 800\n",
        "          show_every = 100\n",
        "\n",
        "        style_weights = {'conv1_1': 0.25,\n",
        "                         'conv2_1': 0.25,\n",
        "                         'conv3_1': 0.25,\n",
        "                         'conv4_1': 0.25,\n",
        "                         'conv5_1': 0} # we do not use conv5_1\n",
        "\n",
        "        content_weight = 50  # alpha\n",
        "        style_weight = 150  # beta\n",
        "        longterm_weight = 750 # gamma\n",
        "\n",
        "        optimizer = optim.Adam([target_image], lr=0.01)\n",
        "        \n",
        "        for ii in range(1, steps+1):\n",
        "            # Get the features from the target image:\n",
        "            target_features = get_features(target_image, styleTransferModel)\n",
        "            \n",
        "            # 1. Calculate the content loss:\n",
        "            batch_size, depth, height, width = content_features['conv4_1'].shape\n",
        "            content_loss = torch.mean((target_features['conv4_1'] - content_features['conv4_1']) ** 2)\n",
        "            \n",
        "            # 2. Calculate the style loss:\n",
        "            style_loss = 0\n",
        "            for layer in style_weights:\n",
        "                target_feature = target_features[layer]\n",
        "                batch_size, depth, height, width = target_feature.shape\n",
        "                target_gram = gram_matrix(target_feature)\n",
        "                style_gram = style_grams[layer]\n",
        "                layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram) ** 2)\n",
        "                style_loss += layer_style_loss / (depth * height * width)\n",
        "\n",
        "            # 3. Calculate the long-term temporal loss:\n",
        "            longterm_loss = 0\n",
        "            if content_image_count == 2:\n",
        "              batch_size, depth, height, width = load_image('../' + content_img_dir + '/warped/frame_%d_%d_warped.jpg' %(content_image_count-1, content_image_count)).shape\n",
        "              temporal_loss = torch.sum(((target_image - load_image('../' + content_img_dir + '/warped/frame_%d_%d_warped.jpg' %(content_image_count-1, content_image_count)).to(device))**2) * \n",
        "                                        (torch.from_numpy(np.repeat(np.load('../flow/flow_matrix/long_pixel_weight_%d_%d.npy' %(content_image_count-1, content_image_count))[:,:,np.newaxis], \n",
        "                                                                    depth, axis=2)).to(device).unsqueeze(0).permute(0,3,1,2))) / (depth * height * width)\n",
        "              longterm_loss += temporal_loss\n",
        "            if ((content_image_count == 3) | (content_image_count == 4)):\n",
        "              batch_size, depth, height, width = load_image('../' + content_img_dir + '/warped/frame_%d_%d_warped.jpg' %(content_image_count-1, content_image_count)).shape\n",
        "              for frame_gap in (1,2):\n",
        "                temporal_loss = torch.sum(((target_image - load_image('../' + content_img_dir + '/warped/frame_%d_%d_warped.jpg' %(content_image_count-frame_gap, content_image_count)).to(device))**2) * \n",
        "                                        (torch.from_numpy(np.repeat(np.load('../flow/flow_matrix/long_pixel_weight_%d_%d.npy' %(content_image_count-frame_gap, content_image_count))[:,:,np.newaxis], \n",
        "                                                                    depth, axis=2)).to(device).unsqueeze(0).permute(0,3,1,2))) / (depth * height * width)\n",
        "                longterm_loss += temporal_loss\n",
        "            if content_image_count > 4:\n",
        "              batch_size, depth, height, width = load_image('../' + content_img_dir + '/warped/frame_%d_%d_warped.jpg' %(content_image_count-1, content_image_count)).shape\n",
        "              for frame_gap in (1,2,4):\n",
        "                temporal_loss = torch.sum(((target_image - load_image('../' + content_img_dir + '/warped/frame_%d_%d_warped.jpg' %(content_image_count-frame_gap, content_image_count)).to(device))**2) * \n",
        "                                        (torch.from_numpy(np.repeat(np.load('../flow/flow_matrix/long_pixel_weight_%d_%d.npy' %(content_image_count-frame_gap, content_image_count))[:,:,np.newaxis], \n",
        "                                                                    depth, axis=2)).to(device).unsqueeze(0).permute(0,3,1,2))) / (depth * height * width)\n",
        "                longterm_loss += temporal_loss\n",
        "\n",
        "            # Calculate the total loss:\n",
        "            total_loss = content_weight * content_loss + style_weight * style_loss + longterm_weight * longterm_loss\n",
        "\n",
        "            # Update the target image:\n",
        "            optimizer.zero_grad()  # zero out the gradients from previous iterations\n",
        "            total_loss.backward()  # Backpropagate the loss\n",
        "            optimizer.step()  # Update the target image\n",
        "\n",
        "            # Display the intermediate results if required:\n",
        "            if ii % show_every == 0:\n",
        "                print(\"Frame %s Total loss: \" %content_image_count, total_loss.item())\n",
        "\n",
        "        # Save the style transferred image:\n",
        "        target_image = img_convert(target_image)\n",
        "        plt.imsave('../output_style_transferred_frames/st_frame_%d.jpg' % content_image_count, target_image)\n",
        "        print(\"completed style transfer on image: \", content_image_count)\n",
        "        content_image_count += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V7JC9XscOWhD",
        "colab": {}
      },
      "source": [
        "# Apply style transfer on the input frames:\n",
        "apply_style_transfer('input_content_frames', 'input_style_frames', 'style_3.jpg', 'output_style_transferred_frames')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GgqJvGuCOZML",
        "colab": {}
      },
      "source": [
        "# Convert the final images to video:\n",
        "def img_to_video(st_output_dir, output_video_name):\n",
        "    img = cv2.imread('../' + st_output_dir + '/st_frame_1.jpg')\n",
        "    height, width, layers = img.shape\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "    video = cv2.VideoWriter('../output_processed_video/%s.avi' %output_video_name, fourcc, 25, (width, height))\n",
        "    for i in range(1, len([name for name in os.listdir('../%s' %st_output_dir) if os.path.isfile(os.path.join('../', st_output_dir, name))])+1):\n",
        "        video.write(cv2.imread('../' + st_output_dir + '/st_frame_' + str(i) + '.jpg'))\n",
        "\n",
        "    cv2.destroyAllWindows()\n",
        "    video.release()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd8ZUNLtt4Q8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_to_video('output_style_transferred_frames', 'style_transfered_video')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TceBsn8Z6j7h",
        "colab_type": "text"
      },
      "source": [
        "#Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yY2laALgkQ7e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "content_image_count = 1\n",
        "content_image = load_image('../input_content_frames/frame_1.jpg')\n",
        "content_img_dir = 'input_content_frames'\n",
        "num_content_imgs = len([name for name in os.listdir('../%s' %content_img_dir) if os.path.isfile(os.path.join('../', content_img_dir, name))])\n",
        "longterm_weight = 750 # gamma\n",
        "content_weight = 50  # alpha\n",
        "style_weight = 150\n",
        "\n",
        "tot_loss = []\n",
        "temp_loss = []\n",
        "\n",
        "while(content_image_count <= num_content_imgs):\n",
        "    content_image = load_image('../' + content_img_dir + '/frame_' + str(content_image_count) + '.jpg').to(device)\n",
        "    style_image = load_image('../' + 'input_style_frames' + '/' + 'style_3.jpg', shape=content_image.shape[-2:]).to(device)\n",
        "    target_image = load_image('../output_style_transferred_frames/st_frame_%d.jpg' % content_image_count, shape=content_image.shape[-2:]).to(device)\n",
        "    #target_image = load_image('../output_frames_with_temporal/st_frame_%d.jpg' % content_image_count, shape=content_image.shape[-2:]).to(device)\n",
        "\n",
        "    content_features = get_features(content_image, styleTransferModel)\n",
        "    style_features = get_features(style_image, styleTransferModel)\n",
        "    target_features = get_features(target_image, styleTransferModel)\n",
        "    \n",
        "    style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n",
        "\n",
        "    style_weights = {'conv1_1': 0.25,'conv2_1': 0.25,'conv3_1': 0.25,'conv4_1': 0.25,'conv5_1': 0} \n",
        "\n",
        "    # 1. Calculate the content loss:\n",
        "    batch_size, depth, height, width = content_features['conv4_1'].shape\n",
        "    content_loss = torch.mean((target_features['conv4_1'] - content_features['conv4_1']) ** 2)\n",
        "\n",
        "    style_loss = torch.tensor(0.)\n",
        "    # iterate through each style layer and add to the style loss:\n",
        "    for layer in style_weights:\n",
        "        # Get the \"target\" style representation for the layer:\n",
        "        target_feature = target_features[layer]\n",
        "        batch_size, depth, height, width = target_feature.shape\n",
        "\n",
        "        # Calculate the target gram matrix:\n",
        "        target_gram = gram_matrix(target_feature)\n",
        "\n",
        "        # Get the \"style\" from the style gram matrices computed earlier:\n",
        "        style_gram = style_grams[layer]\n",
        "\n",
        "        # the style loss for one layer, weighted appropriately:\n",
        "        layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram) ** 2)\n",
        "\n",
        "        # Add to the style loss:\n",
        "        style_loss += layer_style_loss / (depth * height * width)\n",
        "\n",
        "    # 3. Calculate the long-term temporal loss:\n",
        "    longterm_loss = torch.tensor(0.)\n",
        "    if content_image_count == 2:\n",
        "      batch_size, depth, height, width = load_image('../' + content_img_dir + '/warped/frame_%d_%d_warped.jpg' %(content_image_count-1, content_image_count)).shape\n",
        "      temporal_loss = torch.sum(((target_image - load_image('../' + content_img_dir + '/warped/frame_%d_%d_warped.jpg' %(content_image_count-1, content_image_count)).to(device))**2) * \n",
        "                                (torch.from_numpy(np.repeat(np.load('../flow/flow_matrix/long_pixel_weight_%d_%d.npy' %(content_image_count-1, content_image_count))[:,:,np.newaxis], \n",
        "                                                            depth, axis=2)).to(device).unsqueeze(0).permute(0,3,1,2))) / (depth * height * width)\n",
        "      longterm_loss += temporal_loss\n",
        "    if ((content_image_count == 3) | (content_image_count == 4)):\n",
        "      batch_size, depth, height, width = load_image('../' + content_img_dir + '/warped/frame_%d_%d_warped.jpg' %(content_image_count-1, content_image_count)).shape\n",
        "      for frame_gap in (1,2):\n",
        "        temporal_loss = torch.sum(((target_image - load_image('../' + content_img_dir + '/warped/frame_%d_%d_warped.jpg' %(content_image_count-frame_gap, content_image_count)).to(device))**2) * \n",
        "                                (torch.from_numpy(np.repeat(np.load('../flow/flow_matrix/long_pixel_weight_%d_%d.npy' %(content_image_count-frame_gap, content_image_count))[:,:,np.newaxis], \n",
        "                                                            depth, axis=2)).to(device).unsqueeze(0).permute(0,3,1,2))) / (depth * height * width)\n",
        "        longterm_loss += temporal_loss\n",
        "    if content_image_count > 4:\n",
        "      batch_size, depth, height, width = load_image('../' + content_img_dir + '/warped/frame_%d_%d_warped.jpg' %(content_image_count-1, content_image_count)).shape\n",
        "      for frame_gap in (1,2,4):\n",
        "        temporal_loss = torch.sum(((target_image - load_image('../' + content_img_dir + '/warped/frame_%d_%d_warped.jpg' %(content_image_count-frame_gap, content_image_count)).to(device))**2) * \n",
        "                                (torch.from_numpy(np.repeat(np.load('../flow/flow_matrix/long_pixel_weight_%d_%d.npy' %(content_image_count-frame_gap, content_image_count))[:,:,np.newaxis], \n",
        "                                                            depth, axis=2)).to(device).unsqueeze(0).permute(0,3,1,2))) / (depth * height * width)\n",
        "        longterm_loss += temporal_loss\n",
        "\n",
        "    tot_loss.append((longterm_loss*longterm_weight+content_loss*content_weight+style_loss*style_weight).item()) \n",
        "    temp_loss.append(longterm_loss.item()*longterm_weight)   \n",
        "\n",
        "    print(\"completed style transfer on image: \", content_image_count)\n",
        "    content_image_count += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7Z5b2mV6tOv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_loss_dep = tot_loss\n",
        "temp_loss_dep = temp_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKX4yQyB6wfK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp_loss_ind = temp_loss\n",
        "total_loss_ind = tot_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIxwkKEk6yZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(temp_loss_dep[:210], label='dependency temp loss')\n",
        "plt.plot(total_loss_dep[:210], label='dependency total loss')\n",
        "plt.plot(total_loss_ind[:210], label='independent total loss')\n",
        "plt.plot(temp_loss_ind[:210], label='independent temp loss')\n",
        "plt.xlabel('Frame')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Total/temporal loss comparison')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcSZsb8160bR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(np.array(total_loss_dep[:210])-np.array(temp_loss_dep[:210]), label='dependency')\n",
        "plt.plot(np.array(total_loss_ind[:210])-np.array(temp_loss_ind[:210]), label='independent')\n",
        "plt.xlabel('Frame')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Content+Style loss comparison')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGFXmRcj62kC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(temp_loss_dep[:210], label='dependency')\n",
        "plt.plot(temp_loss_ind[:210], label='independent')\n",
        "plt.xlabel('Frame')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Temporal loss comparison')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}